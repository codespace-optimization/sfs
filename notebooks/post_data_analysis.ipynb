{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def comparative_analysis(solution_path_0: str, solution_path_1: str):\n",
    "    '''\n",
    "    Both solution_path_0 and solution_path_1 are paths to the solutions of the same problem, in jsonl format.\n",
    "\n",
    "    Calculate the following metrics:\n",
    "    - Number of solutions where both \"is_solved\" fields are True\n",
    "\n",
    "    Solutions should be compared by their \"task_id\" field.\n",
    "    '''\n",
    "    \n",
    "    solutions_0 = {}\n",
    "    solutions_1 = {}\n",
    "\n",
    "    # Load solutions from the first path\n",
    "    with open(solution_path_0, 'r') as file_0:\n",
    "        for line in file_0:\n",
    "            solution_dict = json.loads(line)\n",
    "            task_id = solution_dict['task_id']\n",
    "            solutions_0[task_id] = solution_dict['is_solved']\n",
    "\n",
    "    # Load solutions from the second path\n",
    "    with open(solution_path_1, 'r') as file_1:\n",
    "        for line in file_1:\n",
    "            solution_dict = json.loads(line)\n",
    "            task_id = solution_dict['task_id']\n",
    "            solutions_1[task_id] = solution_dict['is_solved']\n",
    "\n",
    "    # Calculate the metric: Number of solutions where both \"is_solved\" fields are True\n",
    "    both_solved_true = 0\n",
    "    for task_id in solutions_0:\n",
    "        if task_id in solutions_1:\n",
    "            if solutions_0[task_id] and solutions_1[task_id]:\n",
    "                both_solved_true += 1\n",
    "\n",
    "    return both_solved_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_analysis(\"../data/generated_solutions/humaneval_164_groundtruth/reflexion/solutions.jsonl\", \"../data/generated_solutions/humaneval_164_groundtruth/reflexion-10iters/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_analysis(\"../data/generated_solutions/humaneval_164_groundtruth/reflexion/solutions.jsonl\", \"../data/generated_solutions/humaneval_164_groundtruth/scatter-mcts/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_analysis(\"../data/generated_solutions/humaneval_164_groundtruth/scatter-mcts-7iters-3seeds/solutions.jsonl\", \"../data/generated_solutions/humaneval_164_groundtruth/scatter-mcts/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_analysis(\"../data/generated_solutions/humaneval_164_groundtruth/tree-mcts/solutions.jsonl\", \"../data/generated_solutions/humaneval_164_groundtruth/scatter-mcts/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_analysis(\"../data/generated_solutions/mbpp_groundtruth/reflexion-10/solutions.jsonl\", \"../data/generated_solutions/mbpp_groundtruth/synthesis-mcts-10/solutions.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
